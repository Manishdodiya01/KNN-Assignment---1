{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e24698-4d8e-4ee2-afa2-b8de63c6c8d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834750f-a8e6-4ba0-aa6e-4d81993d4bde",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for classification and regression tasks. It's considered a simple but powerful method for making predictions based on the similarity of data points.\n",
    "\n",
    "Here's how KNN works:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - Store the feature vectors and their corresponding labels from the training data.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - When given a new, unseen data point, the algorithm calculates the distances (often using metrics like Euclidean distance) between this point and all other points in the training set.\n",
    "   - It then identifies the k-nearest data points (the \"neighbors\") based on these distances.\n",
    "\n",
    "3. **Classification Task**:\n",
    "   - For classification tasks, KNN takes a majority vote from the k-nearest neighbors to determine the class of the new data point. In other words, it counts the frequency of each class among the k-nearest neighbors and assigns the class with the highest frequency to the new point.\n",
    "\n",
    "4. **Regression Task**:\n",
    "   - For regression tasks, KNN calculates the average or weighted average of the target values of the k-nearest neighbors and assigns this value to the new data point.\n",
    "\n",
    "**Key Parameters**:\n",
    "- **k**: The number of nearest neighbors to consider. It's a hyperparameter that needs to be tuned; choosing an appropriate value of k is crucial.\n",
    "- **Distance Metric**: The measure used to calculate the distance between data points (e.g., Euclidean distance, Manhattan distance, etc.).\n",
    "\n",
    "**Pros**:\n",
    "- Simple to implement and understand.\n",
    "- Non-parametric (doesn't make assumptions about the underlying data distribution).\n",
    "- Can be used for both classification and regression tasks.\n",
    "\n",
    "**Cons**:\n",
    "- Can be computationally expensive, especially with large datasets.\n",
    "- Sensitive to the choice of distance metric and the value of k.\n",
    "- Doesn't learn the underlying structure of the data, so it might not perform well in complex datasets.\n",
    "\n",
    "KNN is often used as a baseline model to compare with more complex algorithms, or in situations where the data distribution is not well understood. It can be a powerful tool in the right context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb212d-470d-4c54-a467-8a534ca07577",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf3bbe-cded-455a-8025-1d3ff3a4cd4a",
   "metadata": {},
   "source": [
    "Choosing the right value of \\(k\\) in the k-Nearest Neighbors (KNN) algorithm is a crucial step, as it significantly affects the performance of the model. Here are some common methods for selecting an appropriate value of \\(k\\):\n",
    "\n",
    "1. **Odd Values for Binary Classification**:\n",
    "   - For binary classification problems, it's often recommended to choose an odd value of \\(k\\) to avoid ties when voting for the class label. Ties can lead to ambiguous predictions.\n",
    "\n",
    "2. **Cross-Validation**:\n",
    "   - Use techniques like k-fold cross-validation to evaluate the performance of the model for different values of \\(k\\). This involves splitting the training data into \\(k\\) subsets, using \\(k-1\\) of them for training and the remaining one for validation. Repeat this process \\(k\\) times, rotating the validation set each time. Compute the average performance metric (e.g., accuracy, mean squared error) for each value of \\(k\\) and choose the one with the best performance.\n",
    "\n",
    "3. **Grid Search**:\n",
    "   - Perform a grid search over a range of possible values for \\(k\\) and evaluate the model's performance for each value. This is similar to cross-validation but allows you to explicitly specify the range of values to consider.\n",
    "\n",
    "4. **Use Domain Knowledge**:\n",
    "   - Depending on the specific domain and nature of the data, you may have some prior knowledge that suggests a reasonable range for \\(k\\). For example, if you know that the classes are well-separated, you might start with a smaller \\(k\\). Conversely, if the classes are more overlapping, a larger \\(k\\) might be appropriate.\n",
    "\n",
    "5. **Experiment and Iterate**:\n",
    "   - It's often a good idea to try different values of \\(k\\) and observe how the model performs. You can adjust \\(k\\) based on the results and fine-tune it for the best performance.\n",
    "\n",
    "6. **Plotting Error vs. \\(k\\)**:\n",
    "   - Plotting the error (e.g., classification error or mean squared error) as a function of \\(k\\) can provide insights into the relationship between \\(k\\) and model performance. You can visually inspect the plot to find an optimal value for \\(k\\).\n",
    "\n",
    "7. **Consider Computational Resources**:\n",
    "   - Keep in mind that larger values of \\(k\\) will require more computational resources for predictions, as the algorithm needs to calculate distances to a larger number of neighbors.\n",
    "\n",
    "Remember that there is no one-size-fits-all answer for the best value of \\(k\\). It depends on the specific dataset and problem at hand. Experimentation and validation are key to finding an optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f802aa-3f3d-40c0-9800-aaf69f7974b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f35d2-8105-4d6a-8084-4592edb2d065",
   "metadata": {},
   "source": [
    "The main difference between the K-Nearest Neighbors (KNN) classifier and regressor lies in the type of prediction they make and the nature of the target variable.\n",
    "\n",
    "1. **KNN Classifier**:\n",
    "\n",
    "   - **Type of Prediction**: Classification tasks involve assigning a class label to a data point.\n",
    "   \n",
    "   - **Target Variable**: Categorical or discrete. This means the output variable takes on a finite set of values (e.g., classes or categories).\n",
    "\n",
    "   - **Prediction Process**: The KNN classifier predicts the class of a new data point based on the majority class among its k nearest neighbors.\n",
    "\n",
    "   - **Example**: In a binary classification problem, the KNN classifier might be used to predict whether an email is spam (class 1) or not spam (class 0) based on features like word frequency, sender, etc.\n",
    "\n",
    "2. **KNN Regressor**:\n",
    "\n",
    "   - **Type of Prediction**: Regression tasks involve predicting a continuous numerical value.\n",
    "   \n",
    "   - **Target Variable**: Continuous. This means the output variable can take on an infinite number of values within a range.\n",
    "\n",
    "   - **Prediction Process**: The KNN regressor predicts the target value of a new data point based on the average (or weighted average) of the target values of its k nearest neighbors.\n",
    "\n",
    "   - **Example**: Predicting the price of a house based on features like square footage, number of bedrooms, location, etc. This is a regression task because the target variable (price) is a continuous quantity.\n",
    "\n",
    "In summary, the key distinction is that the KNN classifier is used for classification tasks, where the goal is to assign a class label, while the KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "Both algorithms use the same underlying principle of finding the k-nearest neighbors based on some similarity metric (e.g., Euclidean distance) and making predictions based on the information from those neighbors. The difference lies in how they handle the nature of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093c125-260d-4ab6-a1ed-f2b8498afba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. How do you measure the performance of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa78ca9-2788-4d2a-89d8-fe81d75d4d9a",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) model can be evaluated using various metrics depending on whether it's used for classification or regression tasks:\n",
    "\n",
    "For Classification:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances out of the total instances.\n",
    "\n",
    "Precision, Recall, and F1-Score: These metrics are especially useful in imbalanced datasets.\n",
    "\n",
    "Precision: The proportion of true positives out of the total predicted positives.\n",
    "Recall (Sensitivity or True Positive Rate): The proportion of true positives out of the actual positives.\n",
    "F1-Score: The harmonic mean of precision and recall.\n",
    "Confusion Matrix: This provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC): Useful for binary classification problems. ROC curve plots the true positive rate against the false positive rate at various thresholds, and AUC measures the area under the ROC curve.\n",
    "\n",
    "For Regression:\n",
    "\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n",
    "\n",
    "Mean Squared Error (MSE): The average of the squared differences between predicted and actual values.\n",
    "\n",
    "Root Mean Squared Error (RMSE): The square root of the MSE, which gives a sense of the scale of the error in the same units as the target variable.\n",
    "\n",
    "R-Squared (R²): A measure of how well the model explains the variance in the target variable. It ranges from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "Residual Analysis: Plotting the residuals (the differences between predicted and actual values) can provide insights into the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcd897-f0b6-445d-a9a9-c3f534cd2e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d829f-35e0-43c6-a81a-9a04980d214f",
   "metadata": {},
   "source": [
    "The \"Curse of Dimensionality\" refers to a set of problems that arise when working with high-dimensional data in machine learning, including the K-Nearest Neighbors (KNN) algorithm. It's characterized by a series of challenges that occur as the number of features or dimensions in the data increases.\n",
    "\n",
    "Here are some of the key issues associated with the curse of dimensionality:\n",
    "\n",
    "Increased Computational Complexity: As the number of dimensions increases, the number of calculations required to compute distances between data points grows exponentially. This can make the KNN algorithm computationally expensive and slow.\n",
    "\n",
    "Sparse Data: In high-dimensional spaces, data points become increasingly sparse. This means that the distance between neighboring points becomes less meaningful, as most points are far away from each other.\n",
    "\n",
    "Overfitting: With a large number of dimensions, the model can fit the training data very closely, potentially leading to poor generalization to unseen data (overfitting). This is because it's easier to find close neighbors in high-dimensional space, which may not actually be meaningful.\n",
    "\n",
    "Diminishing Returns: Adding more features doesn't always lead to better performance. In fact, beyond a certain point, additional features can introduce noise and redundancy, making it harder for the algorithm to find meaningful patterns.\n",
    "\n",
    "Increased Sample Size Requirement: As the dimensionality increases, the amount of data required to maintain a certain level of performance also increases. This means that more data is needed to effectively train the model.\n",
    "\n",
    "Loss of Intuition: In high-dimensional spaces, it becomes difficult for humans to visualize and understand the relationships between variables.\n",
    "\n",
    "To mitigate the curse of dimensionality, techniques like dimensionality reduction (e.g., Principal Component Analysis) or feature selection can be employed to reduce the number of dimensions while retaining important information. Additionally, using domain knowledge to select relevant features can also help address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689788cd-a0f4-4d61-b1f5-b05e365cd3dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa68288-431b-47b5-955c-f455e8a95b32",
   "metadata": {},
   "source": [
    "Imputation:\n",
    "\n",
    "Fill missing values with estimated values. This could be done using methods like mean imputation (replacing missing values with the mean of the feature) or median imputation (replacing missing values with the median of the feature).\n",
    "Nearest Neighbors Imputation:\n",
    "\n",
    "Use the KNN algorithm to find the \n",
    "�\n",
    "k nearest neighbors of the data point with the missing value. Then, impute the missing value with the average (or weighted average) of the feature from those neighbors.\n",
    "Model-Based Imputation:\n",
    "\n",
    "Train a model (e.g., regression model) to predict missing values based on the other features. Use the model to fill in the missing values.\n",
    "Deletion:\n",
    "\n",
    "Remove data points with missing values. This is a straightforward but potentially costly approach as it reduces the amount of data available for training.\n",
    "Use of Special Values:\n",
    "\n",
    "Sometimes, missing values can be encoded as a specific value (e.g., -1 or NaN) and the algorithm is designed to handle such values appropriately.\n",
    "Predictive Mean Matching:\n",
    "\n",
    "This technique involves predicting the missing values using a regression model and then using the closest observed values (in terms of predicted value) to replace the missing values.\n",
    "Time Series Interpolation:\n",
    "\n",
    "In time series data, missing values can often be interpolated based on the values before and after the missing point.\n",
    "The choice of method depends on the nature of the data, the extent of missingness, and the specific problem at hand. It's important to evaluate the impact of the chosen imputation method on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5ae14-e559-46cd-9dc6-52c4cca95a94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667faac-783d-424a-b3f0-45de530b0c1d",
   "metadata": {},
   "source": [
    "KNN Classifier:\n",
    "\n",
    "Use Case: Used for classification tasks where the goal is to assign a class label to a data point.\n",
    "Output: Predicts a discrete class label.\n",
    "Evaluation Metrics: Accuracy, precision, recall, F1-score, confusion matrix, ROC-AUC, etc.\n",
    "Suitable for: Problems where the target variable is categorical, such as spam detection, image recognition, sentiment analysis, etc.\n",
    "Considerations: Works best when the decision boundaries are well-defined and the classes are separable.\n",
    "KNN Regressor:\n",
    "\n",
    "Use Case: Used for regression tasks where the goal is to predict a continuous numerical value.\n",
    "Output: Predicts a continuous value.\n",
    "Evaluation Metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-Squared (R²), etc.\n",
    "Suitable for: Problems where the target variable is continuous, such as predicting house prices, temperature forecasting, financial forecasting, etc.\n",
    "Considerations: Works well when there is a correlation between features and target variable, and the underlying relationship is relatively smooth.\n",
    "Choosing Between Classifier and Regressor:\n",
    "\n",
    "The choice between a classifier and regressor depends on the nature of the problem and the type of the target variable.\n",
    "If the target variable is categorical (e.g., class labels), use a classifier.\n",
    "If the target variable is continuous (e.g., numeric values), use a regressor.\n",
    "It's important to select the appropriate type to match the nature of the problem, as using the wrong type can lead to inaccurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0166fab-4304-4f74-8ecb-bb03d835e765",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9c544-45ec-4293-b72b-4d5690cbe4e2",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "Simple and Intuitive: Easy to understand and implement.\n",
    "Non-parametric: Doesn't make assumptions about the underlying data distribution.\n",
    "Adapts to Local Patterns: Can capture complex relationships and adapt to local patterns in the data.\n",
    "Can Handle Multiclass Problems: Can be used for both binary and multiclass classification.\n",
    "Useful for Anomaly Detection: Can be effective in detecting outliers or anomalies in the data.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally Expensive: Can be slow, especially with large datasets or high-dimensional feature spaces, as it requires calculating distances to all data points.\n",
    "Sensitive to Noise and Outliers: Outliers or noisy data can have a significant impact on predictions.\n",
    "Hyperparameter Sensitivity: Performance can be highly dependent on the choice of \n",
    "�\n",
    "k and the distance metric used.\n",
    "Requires Sufficient Data: Performs poorly with small datasets, and more data is generally needed to make accurate predictions.\n",
    "Lack of Model Interpretability: Doesn't provide insights into the underlying relationships between features and target variable.\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Optimizing \n",
    "�\n",
    "k: Use techniques like cross-validation or grid search to find an optimal value for \n",
    "�\n",
    "k.\n",
    "Outlier Detection and Handling: Preprocess data to identify and handle outliers before applying KNN.\n",
    "Dimensionality Reduction: Use techniques like Principal Component Analysis (PCA) to reduce the number of dimensions and alleviate computational costs.\n",
    "Data Preprocessing: Normalize or standardize features, handle missing values, and remove irrelevant features to improve performance.\n",
    "Ensemble Methods: Combine multiple KNN models (e.g., using bagging or boosting) to improve robustness and reduce sensitivity to hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2e679-b117-4475-a8f1-b1e144ca0cc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5920b-37c6-435d-9050-1869b3f73c9d",
   "metadata": {},
   "source": [
    "Euclidean Distance:\n",
    "\n",
    "Also known as straight-line distance or L2 distance.\n",
    "It is the length of the shortest path between two points in a straight line.\n",
    "In a 2-dimensional space, the Euclidean distance between points \n",
    "(\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "1\n",
    ")\n",
    "(x \n",
    "1\n",
    "​\n",
    " ,y \n",
    "1\n",
    "​\n",
    " ) and \n",
    "(\n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "2\n",
    ")\n",
    "(x \n",
    "2\n",
    "​\n",
    " ,y \n",
    "2\n",
    "​\n",
    " ) is calculated as \n",
    "(\n",
    "�\n",
    "2\n",
    "−\n",
    "�\n",
    "1\n",
    ")\n",
    "2\n",
    "+\n",
    "(\n",
    "�\n",
    "2\n",
    "−\n",
    "�\n",
    "1\n",
    ")\n",
    "2\n",
    "(x \n",
    "2\n",
    "​\n",
    " −x \n",
    "1\n",
    "​\n",
    " ) \n",
    "2\n",
    " +(y \n",
    "2\n",
    "​\n",
    " −y \n",
    "1\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " .\n",
    "It's sensitive to changes in all dimensions and is influenced by the scale of the features.\n",
    "Manhattan Distance:\n",
    "\n",
    "Also known as city block distance or L1 distance.\n",
    "It is the distance between two points measured along the axis at right angles.\n",
    "In a 2-dimensional space, the Manhattan distance between points \n",
    "(\n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "1\n",
    ")\n",
    "(x \n",
    "1\n",
    "​\n",
    " ,y \n",
    "1\n",
    "​\n",
    " ) and \n",
    "(\n",
    "�\n",
    "2\n",
    ",\n",
    "�\n",
    "2\n",
    ")\n",
    "(x \n",
    "2\n",
    "​\n",
    " ,y \n",
    "2\n",
    "​\n",
    " ) is calculated as \n",
    "∣\n",
    "�\n",
    "2\n",
    "−\n",
    "�\n",
    "1\n",
    "∣\n",
    "+\n",
    "∣\n",
    "�\n",
    "2\n",
    "−\n",
    "�\n",
    "1\n",
    "∣\n",
    "∣x \n",
    "2\n",
    "​\n",
    " −x \n",
    "1\n",
    "​\n",
    " ∣+∣y \n",
    "2\n",
    "​\n",
    " −y \n",
    "1\n",
    "​\n",
    " ∣.\n",
    "It's less sensitive to outliers and differences in scale, making it more robust in certain situations.\n",
    "The choice between Euclidean and Manhattan distance depends on the nature of the data and the problem at hand. Euclidean distance is appropriate when the underlying relationships are well-represented by straight-line distances, while Manhattan distance may be more suitable when movement along axes is more relevant (e.g., in grid-like structures or categorical data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21ec5f-0a1b-47cb-8bad-44d608b01056",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aff11e-c531-4c2e-8575-7c6b81a76d5a",
   "metadata": {},
   "source": [
    "Feature scaling is important in K-Nearest Neighbors (KNN) to ensure that all features contribute equally to the distance calculation. Since KNN relies on measuring distances between data points, features with larger scales can dominate the distance calculation.\n",
    "\n",
    "Two common methods of feature scaling are:\n",
    "\n",
    "Min-Max Scaling (Normalization):\n",
    "\n",
    "Scales features to a specific range, usually [0, 1].\n",
    "Formula: \n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "min\n",
    "(\n",
    "�\n",
    ")\n",
    "max\n",
    "(\n",
    "�\n",
    ")\n",
    "−\n",
    "min\n",
    "(\n",
    "�\n",
    ")\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "max(X)−min(X)\n",
    "X−min(X)\n",
    "​\n",
    " .\n",
    "This method is suitable when the features have a bounded range.\n",
    "Standardization (Z-score Scaling):\n",
    "\n",
    "Scales features to have a mean of 0 and a standard deviation of 1.\n",
    "Formula: \n",
    "�\n",
    "new\n",
    "=\n",
    "�\n",
    "−\n",
    "mean\n",
    "(\n",
    "�\n",
    ")\n",
    "std\n",
    "(\n",
    "�\n",
    ")\n",
    "X \n",
    "new\n",
    "​\n",
    " = \n",
    "std(X)\n",
    "X−mean(X)\n",
    "​\n",
    " .\n",
    "Standardization is useful when the features have different units or when the data is normally distributed.\n",
    "Feature scaling helps to prevent features with larger scales from dominating the distance calculation. It ensures that all features contribute proportionally to the similarity measure used by the KNN algorithm. This, in turn, can lead to more accurate predictions and a better-performing model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497afd6-7460-4a7e-92d8-60c5d00a7aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
